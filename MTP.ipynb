{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeuTpmCsMM7BWvDjBzX6eo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joeyyy09/telugu-english-translation/blob/main/MTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Step 1: Install necessary libraries\n",
        "!pip install transformers[torch] datasets sacrebleu sentencepiece evaluate\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import io\n",
        "import torch\n",
        "\n",
        "# Step 3: Prepare your dataset using hardcoded data\n",
        "data = {\n",
        "    'english': [\n",
        "        \"His legs are long.\",\n",
        "        \"Who taught Tom how to speak French?\",\n",
        "        \"I swim in the sea every day.\",\n",
        "        \"Tom popped into the supermarket on his way home to buy some milk.\",\n",
        "        \"Smoke filled the room.\",\n",
        "        \"Tom and Mary understood each other.\",\n",
        "        \"Many men want to be thin, too.\",\n",
        "        \"We need three cups.\",\n",
        "        \"I warned Tom not to come here.\",\n",
        "        \"You two may leave.\",\n",
        "        \"He feels very happy.\"\n",
        "    ],\n",
        "    'telugu': [\n",
        "        \"అతని కాళ్ళు పొడవుగా ఉన్నాయి.\",\n",
        "        \"టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు?\",\n",
        "        \"నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను.\",\n",
        "        \"టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడు సూపర్ మార్కెట్లోకి ప్రవేశించాడు.\",\n",
        "        \"పొగ గదిని నింపింది.\",\n",
        "        \"టామ్ మరియు మేరీ ఒకరినొకరు అర్థం చేసుకున్నారు.\",\n",
        "        \"చాలా మంది పురుషులు కూడా సన్నగా ఉండాలని కోరుకుంటారు.\",\n",
        "        \"మాకు మూడు కప్పులు అవసరం.\",\n",
        "        \"టామ్‌ను ఇక్కడికి రానివ్వమని హెచ్చరించాను.\",\n",
        "        \"మీరిద్దరూ వెళ్ళవచ్చు.\",\n",
        "        \"అతను చాలా సంతోషంగా ఉన్నాడు.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "print(\"\\nDataset loaded successfully from hardcoded data.\")\n",
        "print(f\"Number of examples: {len(dataset)}\")\n",
        "\n",
        "# --- OPTIMIZATION: Check for GPU and set device ---\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"\\n--- WARNING: GPU not found. This will be very slow. ---\")\n",
        "    print(\"--- Go to Runtime > Change runtime type and select T4 GPU. ---\")\n",
        "    device = -1 # Use CPU\n",
        "else:\n",
        "    print(\"\\nGPU found. Using CUDA for acceleration.\")\n",
        "    device = 0 # Use GPU\n",
        "\n",
        "# Step 4: Load Models and Tokenizers with Optimizations\n",
        "# --- OPTIMIZATION: Use float16 for faster inference on GPU ---\n",
        "torch_dtype = torch.float16 if device == 0 else torch.float32\n",
        "\n",
        "# NLLB-100 model (using nllb-200-distilled-600M as a strong baseline)\n",
        "print(\"\\nLoading NLLB model...\")\n",
        "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "nllb_tokenizer = AutoTokenizer.from_pretrained(nllb_model_name, src_lang=\"tel_Telu\", tgt_lang=\"eng_Latn\")\n",
        "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name, torch_dtype=torch_dtype)\n",
        "nllb_translator = pipeline('translation', model=nllb_model, tokenizer=nllb_tokenizer, src_lang=\"tel_Telu\", tgt_lang=\"eng_Latn\", device=device, batch_size=16)\n",
        "print(\"NLLB model loaded.\")\n",
        "\n",
        "# mBART-50 model\n",
        "print(\"\\nLoading mBART-50 model...\")\n",
        "mbart_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "mbart_tokenizer = AutoTokenizer.from_pretrained(mbart_model_name, src_lang=\"te_IN\", tgt_lang=\"en_XX\")\n",
        "mbart_model = AutoModelForSeq2SeqLM.from_pretrained(mbart_model_name, torch_dtype=torch_dtype)\n",
        "mbart_translator = pipeline('translation', model=mbart_model, tokenizer=mbart_tokenizer, src_lang=\"te_IN\", tgt_lang=\"en_XX\", device=device, batch_size=16)\n",
        "print(\"mBART-50 model loaded.\")\n",
        "\n",
        "# Step 5: Generate Translations\n",
        "def get_translations(translator, dataset):\n",
        "    # Get the source texts (Telugu) for translation\n",
        "    source_texts = [example[\"telugu\"] for example in dataset]\n",
        "    # The pipeline will handle batching automatically with the batch_size parameter\n",
        "    translations = translator(source_texts)\n",
        "    return [t['translation_text'] for t in translations]\n",
        "\n",
        "print(\"\\nGenerating translations with NLLB...\")\n",
        "nllb_translations = get_translations(nllb_translator, dataset)\n",
        "\n",
        "print(\"\\nGenerating translations with mBART-50...\")\n",
        "mbart_translations = get_translations(mbart_translator, dataset)\n",
        "\n",
        "# Step 6: Calculate Scores\n",
        "chrf_metric = evaluate.load(\"chrf\")\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# The reference translations (English) need to be in a list of lists format\n",
        "references = [[example[\"english\"]] for example in dataset]\n",
        "\n",
        "# Calculate scores for NLLB\n",
        "print(\"\\nCalculating scores for NLLB...\")\n",
        "nllb_chrf_score = chrf_metric.compute(predictions=nllb_translations, references=references)\n",
        "nllb_bleu_score = bleu_metric.compute(predictions=nllb_translations, references=references)\n",
        "\n",
        "# Calculate scores for mBART-50\n",
        "print(\"Calculating scores for mBART-50...\")\n",
        "mbart_chrf_score = chrf_metric.compute(predictions=mbart_translations, references=references)\n",
        "mbart_bleu_score = bleu_metric.compute(predictions=mbart_translations, references=references)\n",
        "\n",
        "# Step 7: Display Results\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "print(\"\\nNLLB-100 Scores:\")\n",
        "print(f\"chrF Score: {nllb_chrf_score['score']:.2f}\")\n",
        "print(f\"BLEU Score: {nllb_bleu_score['score']:.2f}\")\n",
        "\n",
        "print(\"\\nmBART-50 Scores:\")\n",
        "print(f\"chrF Score: {mbart_chrf_score['score']:.2f}\")\n",
        "print(f\"BLEU Score: {mbart_bleu_score['score']:.2f}\")\n",
        "\n",
        "# Display translations for a side-by-side comparison\n",
        "results_df = pd.DataFrame({\n",
        "    \"Telugu Source\": df[\"telugu\"],\n",
        "    \"English Reference\": df[\"english\"],\n",
        "    \"NLLB Translation\": nllb_translations,\n",
        "    \"mBART-50 Translation\": mbart_translations\n",
        "})\n",
        "\n",
        "print(\"\\n--- Translation Comparison ---\")\n",
        "# Set pandas display options to show full text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(results_df.to_string())\n"
      ],
      "metadata": {
        "id": "eG3ZLFNkpyis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}